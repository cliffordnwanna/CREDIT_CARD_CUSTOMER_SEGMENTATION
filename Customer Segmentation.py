# -*- coding: utf-8 -*-
"""GOMYCODE CHECKPOINT 23 [Unsupervised Learning Clustering Checkpoint].ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IZLCObjbBsdStzPkx5MYnYvpb-QJW7Es

In this project, i worked on the 'Credit Card Dataset for Clustering' dataset provided by Kaggle.

Dataset description : This dataset was derived and simplified for learning purposes. It includes usage behaviour of about 9000 active credit card holders during 6 months period. This case requires to develop a customer segmentation to define marketing strategy.

Dataset link : https://www.kaggle.com/datasets/arjunbhasin2013/ccdata


Columns explanation :

CUST_ID: Identification of Credit Card holder (Categorical)

BALANCE_FREQUENCY: How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)

PURCHASES: Amount of purchases made from account

CASH_ADVANCE: Cash in advance given by the user

CREDIT_LIMIT: Limit of Credit Card for user

PAYMENTS: Amount of Payment done by user

# Importing Libraries and Loading Data
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import AgglomerativeClustering, KMeans
from scipy.cluster.hierarchy import dendrogram

#import dataset
from google.colab import drive
drive.mount('/content/drive')

#load dataset
df = pd.read_csv("/content/drive/MyDrive/Untitled folder/DATASETS/Credit_card_dataset.csv")

# Make a copy of the original dataframe
df1 = df.copy()

"""# Data Exploration and Preparation

"""

df.head()
#df.tail()

df.info()

df.describe()

df.plot(kind='density', subplots=True, sharex=False, layout=(12,3), figsize=(15,10))
plt.show

# Check for missing values and handle corrupted data if any
# Check for missing values
df.isnull().sum()

# "CREDIT_LIMIT" has 1 missing value.
# drop rows with missing values in the column named 'CREDIT_LIMIT'
df.dropna(subset=['CREDIT_LIMIT'], inplace=True)

df.isnull().sum()

# Handle missing values or corrupted data (e.g., using imputation or removal)

# Remove duplicates, if they exist
df.duplicated().sum()

# The dataframe has no duplicated rows

# CUST_ID contains categorical features,label encoding
# Encode categorical data to convert them into numerical format

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
df['CUST_ID'] = encoder.fit_transform(df['CUST_ID'])

# Handle outliers (optional but can be crucial for clustering)
# Visualization Methods: Box plots, scatter plots, or histograms can visually highlight potential outliers.
# Box plot for outlier visualization of the 5 numerical columns to identify outliers

#sns.boxplot(x=df['CUST_ID'])
#sns.boxplot(x=df['BALANCE_FREQUENCY'])
sns.boxplot(x=df['PURCHASES'])
#sns.boxplot(x=df['PAYMENTS'])
sns.boxplot(x=df['CREDIT_LIMIT'])
#sns.boxplot(x=df['CASH_ADVANCE'])

# Create a scatter plot for 'CREDIT_LIMIT'
plt.figure(figsize=(8, 6))
plt.scatter(range(len(df['CREDIT_LIMIT'])), df['CREDIT_LIMIT'], alpha=0.7, color='blue')
plt.title('Scatter Plot of CREDIT_LIMIT')
plt.xlabel('Index')
plt.ylabel('CREDIT_LIMIT values')
plt.grid(True)
plt.show()

plt.figure(figsize=(8, 6))
plt.scatter(range(len(df['PURCHASES'])), df['PURCHASES'], alpha=0.7, color='blue')
plt.title('Scatter Plot of PURCHASES')
plt.xlabel('Index')
plt.ylabel('PURCHASES')
plt.grid(True)
plt.show()

from scipy import stats

# Define a function to handle outliers using the Z-score method
def handle_outliers_zscore(df, columns, z_threshold=3):
    for col in columns:
        z_scores = np.abs(stats.zscore(df[col]))
        outliers = df[(z_scores > z_threshold)]
        df = df[(z_scores <= z_threshold)]
        print(f"Outliers removed in column '{col}': {len(outliers)}")
    return df

# Define the columns you want to handle outliers for
columns_to_handle_outliers = ['PURCHASES', 'CREDIT_LIMIT']

# Handle outliers in the specified columns
df_cleaned = handle_outliers_zscore(df, columns_to_handle_outliers)

# df_cleaned now contains the DataFrame with outliers removed

# i will proceed with further analysis and modeling using df_cleaned

df_cleaned.head()
df_cleaned.describe()

# Create a scatter plot for 'CREDIT_LIMIT' after handling outliers
plt.figure(figsize=(8, 6))
plt.scatter(range(len(df['CREDIT_LIMIT'])), df['CREDIT_LIMIT'], alpha=0.7, color='blue')
plt.title('Scatter Plot of CREDIT_LIMIT')
plt.xlabel('Index')
plt.ylabel('CREDIT_LIMIT values')
plt.grid(True)
plt.show()

# Create a scatter plot for 'PURCHASES' after handling outliers
plt.figure(figsize=(8, 6))
plt.scatter(range(len(df_cleaned['PURCHASES'])), df_cleaned['PURCHASES'], alpha=0.7, color='blue')
plt.title('Scatter Plot of PURCHASES')
plt.xlabel('Index')
plt.ylabel('PURCHASES')
plt.grid(True)
plt.show()

"""Hierarchical Clustering

## Scaling
Before applying Hierarchical Clustering, we have to normalize the data so that the scale of each variable is the same. Why is this important?. Normalization is a preprocessing technique used to transform data before feeding it into machine learning models

Normalization (normalize function): Normalization rescales each sample (row in the dataset) independently to have a unit norm.
The norm calculation is done on a per-sample basis, ensuring that each row's values are rescaled proportionally such that the Euclidean norm (L2 norm) of each row becomes 1.
It's suitable for scenarios where the direction or relative relations of the samples' features are more important than their magnitudes.

So, let's normalize the data and bring all the variables to the same scale
"""

from sklearn.preprocessing import normalize
df_scaled = normalize(df_cleaned)

df_scaled

""" Next, i converted the normalized data stored in "df_scaled" and create a new DataFrame "df_scaled" with the same column names as the original DataFrame "df_cleaned". This DataFrame will be utilized for further analysis."""

df_scaled = pd.DataFrame(df_scaled, columns=df_cleaned.columns)
df_scaled.head()

df_scaled.describe()

"""# Feature Selection"""

# Selecting features for clustering
selected_features = ['PURCHASES', 'CREDIT_LIMIT']
X = df_scaled[selected_features]

"""Creating a Dendrogram and Identifying the numbers of clusters

After feature selection, we can see that the scale of all the variables is almost similar. Now, we are good to go. Next,I plotted the dendrogram to help decide the number of clusters.

"""

import scipy.cluster.hierarchy as shc
plt.figure(figsize=(10, 7))
plt.title("Dendrograms")
plt.xticks(rotation = 45)
dend = shc.dendrogram(shc.linkage(X, method="ward"))
plt.show()

"""The X-axis contains the samples and y-axis represents the distance between these samples. The vertical line with maximum distance is the blue line and hence we can decide a threshold of 15 and cut the dendrogram."""

plt.figure(figsize=(10, 7))
plt.title("Dendrograms")
dend = shc.dendrogram(shc.linkage(X, method="ward"))
plt.axhline(y=15, color= "r", linestyle= "--")

"""We have two clusters as this line cuts the denndrogram at two points. Let's now apply hierarchical clustering for 2 clusters."""

# Perform hierarchical clustering
hc = AgglomerativeClustering(n_clusters=2, linkage='ward')  # Remove the 'affinity' parameter
cluster_labels = hc.fit_predict(X)  # Fit the model and predict cluster labels

#The fit_predict() method fits the hierarchical clustering algorithm to the dataset X and returns the cluster labels assigned to each data point.

"""cluster.labels_ is an attribute that stores these cluster assignments. It's a 1-dimensional array or list where each element corresponds to a data point in your dataset, indicating which cluster that particular data point belongs to"""

hc.labels_

"""We can see the values of 0s and 1s in the outtput since we defined 2 clusters. 0 represents the points that belong to the first cluster and 1 represents the points in the second cluster. Let's now visualize the two clusters:"""

# Plot the clusters (for 2D visualization)
plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=hc.labels_, cmap='viridis')
plt.xlabel('PURCHASES')
plt.ylabel('CREDIT_LIMIT')
plt.title('Hierarchical Clustering')
plt.show()

"""Awessome! We can clearly visualize the two clusters here. This is how we can implement hierarchical clustering in Python
Hierarchical clustering is a super useful way of segmenting observations.

# K-MEANS CLUSTERING

The first step in performing customer segmentation using KMEANS Clustering algorithmn is choosing the number of clusters. To achieve this, i'd use WCSS

WCSS -> Within Clusters Sum of Squares

Within cluster sum of squares:

The sum of the squared deviations from each observation and the cluster centroid.
Interpretation The within-cluster sum of squares is a measure of the variability of the observations within each cluster. In general, a cluster that has a small sum of squares is more compact than a cluster that has a large sum of squares. Clusters that have higher values exhibit greater variability of the observations within the cluster.

However, similar to sums of squares and mean squares in ANOVA, the within-cluster sum of squares is influenced by the number of observations. As the number of observations increases, the sum of squares becomes larger. Therefore, the within-cluster sum of squares is often not directly comparable across clusters with different numbers of observations. To compare the within-cluster variability of different clusters, use the average distance from centroid instead
"""

for i in range(1, 11):
    print(i)

# finding wcss(minimum) value for different number of clusters

wcss = []

for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init= "k-means++", random_state=42)
    kmeans.fit(X)
    print(kmeans.inertia_)

    wcss.append(kmeans.inertia_)# Getting a WSS value for each cluster

"""# The Elbow Graph/Method

The Elbow Method is a heuristic technique used to determine the optimal number of clusters (K) in a dataset for clustering algorithms like K-means. It helps in identifying the point where the addition of more clusters does not explain much more variance in the data.

Here's how the Elbow Method works:

Variance Explained vs. Number of Clusters (K): The Elbow Method involves plotting the variance (often called inertia or distortion) as a function of the number of clusters.

Inertia or Distortion: In K-means clustering, inertia (or within-cluster sum of squares) measures how internally coherent clusters are. It quantifies the sum of squared distances between each data point and its centroid within a cluster.

Elbow Point: When the number of clusters (K) increases, the inertia typically decreases, as each cluster becomes more compact. However, at a certain point, the rate of decrease sharply changes, forming an 'elbow' shape in the plot.

Optimal K: The optimal number of clusters is often considered to be at the 'elbow' of the plot. It's the point where the rate of decrease in inertia sharply decreases and adding more clusters doesn't significantly reduce inertia.

The goal is to choose the smallest number of clusters that still captures a significant amount of variance in the dataset, avoiding overly complex models that may overfit or add little interpretability.

The Elbow Method provides an intuitive and visual way to estimate the appropriate number of clusters, but it might not always show a clear elbow. In such cases, domain knowledge or additional evaluation methods may be necessary to determine the optimal number of clusters for a specific dataset.
"""

# plot an elbow graph
sns.set()
plt.plot(range(1, 11), wcss)
plt.title("The Elbow Point Graph")
plt.xlabel("Number of Clusters")
plt.ylabel("WCSS");

"""Optimal Number of Clusters = 4

#### Training the k-Means Clustering Model
"""

# Feature Selection
X = df_cleaned.iloc[:, [2,4]].values

X

# K-means clustering
kmeans = KMeans(n_clusters=4, init="k-means++", random_state=42)

# return a label for each data point based on their cluster
Y = kmeans.fit_predict(X)

Y

Y.shape

import numpy as np

unique_values = np.unique(Y)
count_unique = len(unique_values)

print("Number of unique values:", count_unique)

"""Visualizing all the Clusters"""

# plotting all the clusters and their Centroids

plt.figure(figsize=(6,6))
plt.scatter(X[Y==0,0], X[Y==0,1], s=50, c="green", label="Cluster 1")
plt.scatter(X[Y==1,0], X[Y==1,1], s=50, c="red", label="Cluster 2")
plt.scatter(X[Y==2,0], X[Y==2,1], s=50, c="yellow", label="Cluster 3")
plt.scatter(X[Y==3,0], X[Y==3,1], s=50, c="violet", label="Cluster 4")
#plt.scatter(X[Y==4,0], X[Y==4,1], s=50, c="blue", label="Cluster 5")

# plot the centroids
plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], s=100, c="cyan", label="Centroids")

plt.title("Customer Groups")
plt.xlabel("PURCHASES")
plt.ylabel("CREDIT LIMIT");

"""#Interpretation of Results.
There are 4 segmentations of customers and an approximate amount they spend on purchases and their credit_limit

1. Customers that spend [PURCHASES (0 - 5500)] and credit score[CREDIT SCORE(0 - 3000 )].
2. Customers that spend [PURCHASES (0 - 7500)] and credit score[CREDIT SCORE(3001 - 5800 )].
3. Customers that spend [PURCHASES (0 - 7500)] and credit score[CREDIT SCORE(5800 - 9400 )].
4. Customers that spend [PURCHASES (0 - 7500)] and credit score[CREDIT SCORE(9500 - 14500 )].

from the above results, i drew the following conclusions:
1. The maximum amount spent on purchases is 7500
2. There is a linear relationship between purchases and credit limit. customers with a lower credit limit make more purchases
"""

